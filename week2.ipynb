{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName(\"csvex\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspath=\"/home/lplab/Desktop/210962122/y.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(cspath,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Integer|Square|\n",
      "+-------+------+\n",
      "|      1|     1|\n",
      "|      2|     4|\n",
      "|      3|     9|\n",
      "|      4|    16|\n",
      "|      5|    25|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col\n",
    "fdf=df.filter(col(\"Square\")>=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Integer|Square|\n",
      "+-------+------+\n",
      "|      4|    16|\n",
      "|      5|    25|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf=df.withColumn(\"Square\",col(\"Square\").cast(\"Float\"))\n",
    "cdf=df.withColumn(\"Square\",col(\"Square\")*10)\n",
    "bdf=df.withColumn(\"cSquare\",col(\"Square\")*-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Integer|Square|\n",
      "+-------+------+\n",
      "|      1|   1.0|\n",
      "|      2|   4.0|\n",
      "|      3|   9.0|\n",
      "|      4|  16.0|\n",
      "|      5|  25.0|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Integer|Square|\n",
      "+-------+------+\n",
      "|      1|    10|\n",
      "|      2|    40|\n",
      "|      3|    90|\n",
      "|      4|   160|\n",
      "|      5|   250|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Integer|Square|cSquare|\n",
      "+-------+------+-------+\n",
      "|      1|     1|     -1|\n",
      "|      2|     4|     -4|\n",
      "|      3|     9|     -9|\n",
      "|      4|    16|    -16|\n",
      "|      5|    25|    -25|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(Square)|\n",
      "+-------------+\n",
      "|            2|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "fdf.select(count(fdf.Square)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Integer|Square|\n",
      "+-------+------+\n",
      "|      4|    16|\n",
      "|      5|    25|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg,sum\n",
    "x=df.agg(avg(\"Square\").alias(\"AVG\"),sum(\"Square\").alias(\"Sq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "| AVG| Sq|\n",
      "+----+---+\n",
      "|11.0| 55|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+-----+-------+\n",
      "| NAME|INTEGER|\n",
      "+-----+-------+\n",
      "|  TWO|      2|\n",
      "|  ONE|      1|\n",
      "|THREE|      3|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"TWO\", 2),\n",
    "        (\"ONE\", 1),\n",
    "        (\"THREE\", 3)]\n",
    "schema = [\"NAME\", \"INTEGER\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "output_path = \"/home/lplab/Desktop/210962122\"\n",
    "\n",
    "df_single_partition = df.repartition(1)\n",
    "df_single_partition.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count:\n",
      "TWO,2: 1\n",
      "NAME,INTEGER: 1\n",
      "ONE,1: 1\n",
      "THREE,3: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "input_path=\"/home/lplab/Desktop/210962122/part-00000-68922a37-9f13-48ff-a8fe-6d948c8dcd6f-c000.csv\"\n",
    "lines = spark.sparkContext.textFile(input_path)\n",
    "\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                   .map(lambda word: (word, 1)) \\\n",
    "                   .reduceByKey(add)\n",
    "\n",
    "print(\"Word Count:\")\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
